====================================
 Keras Network Wavefunction Example
====================================

The following scripts are generated by using :code:`wfns_make_script.py` and by tweaking the
generated script. For more information on using :code:`wfns_make_script.py`, go to
:ref:`Using a script that makes a script <tutorial_calc_make_script>` for the tutorial and
:ref:`wfns_make_script.py <script_make_script>` for the API. For more information on customizing the
script, go to :ref:`How to run a calculation by making a script <tutorial_calc_code>`.

For more information, see :py:class:`KerasNetwork <wfns.wfn.network.keras_network.KerasNetwork>`.

KerasNetwork Configuration
--------------------------

Wavefunction
   KerasNetwork
Hamiltonian
   Restricted Chemical Hamiltonian
Optimized Parameters
   Orbitals are optimized
   KerasNetwork weights are optimized
Objective
   Energy calculated using all Slater determinants
Optimizer
   CMA solver

.. code:: python

    import numpy as np
    import os
    from wfns.wfn.network.keras_network import KerasNetwork
    from wfns.ham.restricted_chemical import RestrictedChemicalHamiltonian
    from wfns.backend.sd_list import sd_list
    from wfns.schrodinger.schrodinger.onesided_energy import OneSidedEnergy
    from wfns.solver.equation import cma


    # Number of electrons
    nelec = 4
    print('Number of Electrons: {}'.format(nelec))

    # Number of spin orbitals
    nspin = 8
    print('Number of Spin Orbitals: {}'.format(nspin))

    # One-electron integrals
    one_int_file = 'oneint.npy'
    one_int = np.load(one_int_file)
    print('One-Electron Integrals: {}'.format(os.path.abspath(one_int_file)))

    # Two-electron integrals
    two_int_file = 'twoint.npy'
    two_int = np.load(two_int_file)
    print('Two-Electron Integrals: {}'.format(os.path.abspath(two_int_file)))

    # Nuclear-nuclear repulsion
    nuc_nuc = 0.0
    print('Nuclear-nuclear repulsion: {}'.format(nuc_nuc))

    # Initialize wavefunction
    wfn = KerasNetwork(nelec, nspin, model=None, params=None, dtype=None, memory=None)
    print('Wavefunction: KerasNetwork')

    # Initialize Hamiltonian
    ham = RestrictedChemicalHamiltonian(one_int, two_int, energy_nuc_nuc=nuc_nuc, params=None)
    print('Hamiltonian: RestrictedChemicalHamiltonian')

    # Projection space
    pspace = sd_list(nelec, nspin//2, num_limit=None, exc_orders=[1, 2], spin=None,
                    seniority=wfn.seniority)
    print('Projection space (orders of excitations): [1, 2]')

    # Select parameters that will be optimized
    param_selection = [(wfn, np.ones(wfn.nparams, dtype=bool)), (ham, np.ones(ham.nparams, dtype=bool))]

    # Initialize objective
    objective = OneSidedEnergy(wfn, ham, param_selection=param_selection, refwfn=None)

    # Solve
    print('Optimizing wavefunction: cma solver')
    results = cma(objective, save_file='', sigma0=0.01, options={'ftarget': None, 'timeout': np.inf,
                  'tolfun': 1e-11, 'verb_filenameprefix': 'outcmaes', 'verb_log': 0})

    # Results
    if results['success']:
        print('Optimization was successful')
    else:
        print('Optimization was not successful: {}'.format(results['message']))
    print('Final Energy: {}'.format(results['energy']))

Different Networks
------------------
The default network used is a feed-forward network with two hidden layers. The input is the
occupation of each of the spin orbitals (`1` or `0`). The number of hidden units for each layer is
the number of spin orbitals. There are no bias by default. All of the activation functions are the
rectified linear unit (ReLU). Apart from the input and the output (overlap of the given Slater
determinant), the network's structure can be modified using the Keras API. To use a different
network, build the desired model (:code:`keras.engine.training.Model`) and assign it to the
wavefunction. For example,

.. code:: python

    model = keras.engine.sequential.Sequential()
    model.add(keras.layers.core.Dense(nspin, activation=keras.activations.relu, input_dim=nspin,
                                      use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin * 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin * 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin / 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin / 0.7), activation=keras.activations.softmax,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(1, activation=keras.activations.linear,
                                      input_dim=nspin, use_bias=True)
    wfn = KerasNetwork(nelec, nspin, model=model, params=None, dtype=None, memory=None)

Please note that the random initial guess commonly used when training neural networks will not be
feasible here because we aim to find a specific eigenstate (e.g. lowest energy). The default initial
guess is created only for multi-layer perceptrons with only one type of weights for a layer (i.e. no
bias) and the number of hidden units in the last hidden layer is suitably larger than the number of
electrons. To elaborate, if we treat the last hidden layer as a set of spin orbitals, the number of
first and second order excitations must be greater than the number of units. If the default initial
guess cannot be generated for the given model, then the user must provide it.

For documentation on Keras, see `Keras Documentation <https://keras.io/>`_.
